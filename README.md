We develop an AI-based synthetic contrast-enhanced mammography image generation system, RISS, that can generate recombined images based on low-energy images and successfully apply it to mammography. RISS was trained and evaluation in a large-scale cohort of over one hundred thousand images form six medical hospitals in China and two international public cohorts. Comprehensive quantitative and qualitative evaluations have demonstrated the quality and clinical value of the synthesized recombined images generated by RISS. Importantly, when extended to mammography, RISS eliminated the need for contrast agents and the associated 20–30 minutes of preparation, injection, and observation. This capability significantly enhanced diagnostic sensitivity, reducing the risk of missed diagnoses, and reduced unnecessary biopsies compared to mammography alone.


Installation
First clone the repo and cd into the directory:

git clone https://github.com/yyyhd/RISS
Create a new enviroment with anaconda.

conda create -n RISS python=3.7 -y --no-default-packages
conda activate RISS
pip install --upgrade pip
pip install -r requirements.txt
pip install -e .
Model Download
The MUSK models can be accessed from HuggingFace Hub.

You need to agree to the terms to access the models and login with your HuggingFace write token:

from huggingface_hub import login
login(<huggingface write token>)
Basic Usage: MUSK as a Vision-Language Encoder
Please refer to demo.ipynb for a demonstration.

Load the MUSK model
import torch
from musk import utils, modeling
from timm.models import create_model
model = create_model("musk_large_patch16_384")
utils.load_model_and_may_interpolate("hf_hub:xiangjx/musk", model, 'model|module', '')
model.to(device="cuda", dtype=torch.float16)
model.eval()
Encode images with MUSK (refer to demo.ipynb for complete implementation)
import torchvision
from PIL import Image
from timm.data.constants import IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD

transform = torchvision.transforms.Compose([
    torchvision.transforms.Resize(384, interpolation=3, antialias=True),
    torchvision.transforms.CenterCrop((384, 384)),
    torchvision.transforms.ToTensor(),
    torchvision.transforms.Normalize(mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)
])

img = Image.open('assets/lungaca1014.jpeg').convert("RGB")  # input image
img_tensor = transform(img).unsqueeze(0)
with torch.inference_mode():
    image_embeddings = model(
        image=img_tensor.to("cuda", dtype=torch.float16),
        with_head=False,
        out_norm=False,
        ms_aug=True,
        return_global=True  
        )[0]  # return (vision_cls, text_cls)
with_head=True: Enable head for image-text retrieval.
out_norm=True: Apply normalization.
ms_aug=True: Use multiscale augmentation (for tasks, e.g., linear probe classification, MIL).
return_global=True: Return only [CLS] token, exclude patch tokens.
Encode texts with MUSK (refer to demo.ipynb for complete implementation)
tokenizer = XLMRobertaTokenizer("./musk/models/tokenizer.spm")
text = ['histopathology image of lung adenocarcinoma']
txt_ids, pad = utils.xlm_tokenizer(txt, tokenizer, max_len=100)

with torch.inference_mode():
   text_embeddings = model(
      text_description=txt_ids,
      padding_mask=pad,
      with_head=False, 
      out_norm=True,
      ms_aug=False,
      return_global=True 
   )[1]  # return (vision_cls, text_cls)
Evaluation on Patch-level Benchmarks
Please refer to ./benchmarks/demo.ipynb for a demonstration.

Patch-level benchmarks include image-text retrieval, zero-shot/few-shot/linear probe image classification, image-image retrieval, and more. The evaluation code is all-in-one which adapted from the CLIP Benchmark.

The evaluated dataset includes:

PathMMU is available at https://huggingface.co/datasets/jamessyx/PathMMU.
BookSet and PubmedSet are available at https://warwick.ac.uk/fac/cross_fac/tia/data/arch.
PatchCamelyon can be accessed at https://patchcamelyon.grand-challenge.org/.
NCT-CRC-HE-100K dataset is available at https://zenodo.org/record/1214456.
SICAPv2 can be downloaded from https://data.mendeley.com/datasets/9xxm58dvs3/1.
Osteo dataset is available at https://www.cancerimagingarchive.net/collection/osteosarcoma-tumor-assessment/.
RenalCell can be downloaded from https://zenodo.org/records/6528599.
SkinCancer is accessible at https://www.isic-archive.com/.
LC25000 dataset is available for download at https://github.com/tampapath/lung_colon_image_set.
PanNuke can be accessed at https://warwick.ac.uk/fac/cross_fac/tia/data/pannuke.
UniToPatho dataset is available at https://ieee-dataport.org/open-access/unitopatho.
WSSS4LUAD can be downloaded from https://wsss4luad.grand-challenge.org/WSSS4LUAD/.
BRACS datasets for 3 and 6 classes are available for download at https://www.bracs.icar.cnr.it/.
To begin, download the required datasets. For demonstration purposes, we provide example datasets available here. The UniToPatho dataset has been downsampled to 224×224 resolution to enable faster downloads, as the original dataset exceeds 300GB in size. (As such, performance may vary slightly compared to the results reported in the paper.) Once downloaded, unzip the dataset to a local directory, such as /root/to/downstreams_demo. Next, update the directory path to dataset_root=/root/to/downstreams_demo. The code will then automatically extract features and carry out evaluations.

The main file is clip_benchmark.cli and includes the following options:

--pretrained_model: Specifies the model name and the path to its weights.
--dataset: Indicates the evaluation dataset(s); multiple datasets can be specified.
--dataset_root: The root of datasets.
--task: Defines the evaluation task.
--batch_size: Sets the batch size for feature extraction.
--output: Specifies where to save the output results.
Set the models.txt file with entries in the format: (model_name, model_path). If you want to run both MUSK and CONCH for comparison, your models.txt might look like this:

musk_large_patch16_384,hf_hub:xiangjx/musk
conch,/path/to/conch.pt
Alternatively, you can remove the CONCH entry and run MUSK alone.

Some example commands:

# >>>>>>>>>>> zero-shot image-text retrieval >>>>>>>>>>> #
 python3 -m clip_benchmark.cli eval --pretrained_model models.txt \
        --dataset   "pathmmu_retrieval"  \
        --task "zeroshot_retrieval" \
        --batch_size 256 \
        --num_workers 8 \
        --seed 42 \
        --recall_k 1 10 50 \
        --dataset_root "/root/to/downstreams_demo" \
        --output "./results/benchmark_mm_retrieval.json"
# >>>>>>>>>>> few-shot image classification >>>>>>>>>>> #
for k_shot in "${shot_list[@]}"
do
  for seed in "${seed_list[@]}"
  do
      python3 -m clip_benchmark.cli eval --pretrained_model models.txt \
          --dataset  "skin" "pannuke" "unitopatho" \
          --task "linear_probe" \
          --batch_size 256 \
          --num_workers 8 \
          --fewshot_k $k_shot \
          --seed $seed \
          --dataset_root "/root/to/downstreams_demo" \
          --output "./results/benchmark_fs_${k_shot}shot_seed${seed}.json"
  done
done
# >>>>>>>>>>> zero-shot image2image retrieval >>>>>>>>>>> #
python3 -m clip_benchmark.cli eval --pretrained_model models.txt \
        --dataset   "unitopatho_retrieval" \
        --task "image_retrieval" \
        --batch_size 256 \
        --num_workers 8 \
        --seed 41 \
        --dataset_root "/root/to/downstreams_demo" \
        --output "./results/benchmark_image_retrieval.json"
and more tasks in ./benchmarks/demo.ipynb.

Acknowledgements
The project was built on many amazing open-source repositories: Quilt1M, PathAsst, torchscale, accelerate (model pretraining), deepspeed (model pretraining), pytorch-lightning (downstream finetuning), CLIP Benchmark (model evaluation), and PORPOISE (prognosis). We thank the authors and developers for their contributions.

Issues
Please open new threads or address questions to xiangjx@stanford.edu or xiyue.wang.scu@gmail.com
License
This model and associated code are released under the CC-BY-NC-ND 4.0 license and may only be used for non-commercial, academic research purposes with proper attribution. Any commercial use, sale, or other monetization of the MUSK model and its derivatives, which include models trained on outputs from the MUSK model or datasets created from the MUSK model, is prohibited and requires prior approval.
